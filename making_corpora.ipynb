{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T19:36:04.926664Z",
     "start_time": "2017-08-13T19:36:01.340196Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import vk_api\n",
    "import os\n",
    "import pymorphy2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T19:36:04.932369Z",
     "start_time": "2017-08-13T19:36:04.928742Z"
    }
   },
   "outputs": [],
   "source": [
    "# numpy\n",
    "import numpy\n",
    "\n",
    "# random\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T19:36:04.945529Z",
     "start_time": "2017-08-13T19:36:04.936374Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VK_TOKEN = \"2be03a7932fccb910597649a6a9b2609f1931d809c6f4278c0ed1d50f6774ecd792611d900206b10dcdb1\"\n",
    "FB_TOKEN = \"EAABtRlqbz5YBAFuHv18RyKN8ZCJiMZARqzPsfCADT4eZApjulZA9hxcBt0vaILFY9jRfgO5KX6ZBsM27xUn3kMmFQQXcGpCYFLjEnVZB3ObRN4dZAXaiOOK0OyJogthersprvYNrzkncCKwoDS7116DRIwm8a9Dead7Ef0gVpxa8m5vxSTrcCRt9IQbGMs8EE8VqM9kZANo6bAZDZD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T19:36:05.049766Z",
     "start_time": "2017-08-13T19:36:04.951098Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_posts_fb(user='BillGates'):\n",
    "\n",
    "    # You'll need an access token here to do anything.  You can get a temporary one\n",
    "    # here: https://developers.facebook.com/tools/explorer/\n",
    "    path = f\"corpora_from_fb_users/{user}.txt\"\n",
    "    if not os.path.exists(path):\n",
    "        access_token = FB_TOKEN\n",
    "\n",
    "        graph = facebook.GraphAPI(access_token)\n",
    "        profile = graph.get_object(user)\n",
    "        posts = graph.get_connections(profile['id'], 'posts')\n",
    "        \n",
    "        with open(path, 'w') as f:\n",
    "            texts = []\n",
    "            while True:\n",
    "                try:\n",
    "                    # Perform some action on each post in the collection we receive from\n",
    "                    # Facebook.\n",
    "                    for post in posts['data']:\n",
    "                        msg = post.get('message', '')\n",
    "                        if msg:\n",
    "                            processed_text = full_process(msg)\n",
    "                            texts.append(processed_text)\n",
    "                            _ = f.write(f\"{processed_text}\\n\")\n",
    "                    # Attempt to make a request to the next page of data, if it exists.\n",
    "                    posts = requests.get(posts['paging']['next']).json()\n",
    "                except KeyError:\n",
    "                    # When there are no more pages (['paging']['next']), break from the\n",
    "                    # loop and end the script.\n",
    "                    break\n",
    "        return texts\n",
    "    else:\n",
    "        with open(path) as f:\n",
    "            return list(f)\n",
    "        \n",
    "def get_posts_fb_temp(user='BillGates'):\n",
    "    path = f\"corpora_from_fb_users/{user}.txt\"\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, 'w') as f:\n",
    "            texts = []\n",
    "            for post in t[user]:\n",
    "                if post:\n",
    "                    _ = f.write(f\"{post}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T21:41:25.275694Z",
     "start_time": "2017-08-13T21:41:25.068359Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getallwall(kwargs, n=None):\n",
    "    '''Get all texts from wall generator'''\n",
    "    vk_session = vk_api.VkApi(token=VK_TOKEN)\n",
    "    tools = vk_api.VkTools(vk_session)\n",
    "    if n:\n",
    "        try:\n",
    "            wall_posts = tools.get_all_iter(\"wall.get\", 75, values=kwargs, limit=n)\n",
    "            for i, post in enumerate(wall_posts, 1):\n",
    "                if i > n:\n",
    "                    break\n",
    "                yield post['text']\n",
    "        except:\n",
    "            wall_posts = tools.get_all_iter(\"wall.get\", 15, values=kwargs, limit=n)\n",
    "            for i, post in enumerate(wall_posts, 1):\n",
    "                if i > n:\n",
    "                    break\n",
    "                yield post['text']\n",
    "    else:\n",
    "        try:\n",
    "            wall_posts = tools.get_all_iter(\"wall.get\", 75, values=kwargs)\n",
    "            for post in wall_posts:\n",
    "                yield post['text']\n",
    "        except:\n",
    "            wall_posts = tools.get_all_iter(\"wall.get\", 15, values=kwargs)\n",
    "            for post in wall_posts:\n",
    "                yield post['text']\n",
    "\n",
    "def get_posts_vk_user(user, n_wall=None):\n",
    "    path = f\"corpora_from_vk_users/{user}.txt\"\n",
    "    if not os.path.exists(path):\n",
    "        wall = getallwall({\"owner_id\": user}, n_wall)\n",
    "        with open(path, \"w\") as f:\n",
    "            for post in wall:\n",
    "                processed_text = full_process(post)\n",
    "                if processed_text:\n",
    "                    texts.append(processed_text)\n",
    "                    _ = f.write(f\"{processed_text}\\n\")\n",
    "        return texts\n",
    "    else:\n",
    "        with open(path) as f:\n",
    "            return list(f)\n",
    "        \n",
    "def process_public_vk(public_id, n_wall=None):\n",
    "    texts = []\n",
    "    path = f\"corpora_from_vk_publics/{public_id}.txt\"\n",
    "    if not os.path.exists(path):\n",
    "        wall = getallwall({\"owner_id\": -public_id}, n_wall)\n",
    "        with open(path, \"w\") as f:\n",
    "            for post in wall:\n",
    "                processed_text = full_process(post)\n",
    "                if processed_text:\n",
    "                    texts.append(processed_text)\n",
    "                    _ = f.write(f\"{processed_text}\\n\")\n",
    "        return texts\n",
    "    else:\n",
    "        with open(path) as f:\n",
    "            return list(f)\n",
    "        \n",
    "def get_publics(user_id, num_publics):\n",
    "    vk_session = vk_api.VkApi(token=VK_TOKEN)\n",
    "    vk = vk_session.get_api()\n",
    "    groups = vk.groups.get(user_id=user_id, extended=1, fields='members_count', count=25)['items']\n",
    "    return [g['id'] for g in groups if g.get('members_count', 1000000) < 1000000][:num_publics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T19:36:05.328624Z",
     "start_time": "2017-08-13T19:36:05.181299Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_links(url):\n",
    "    base_url = \"/\".join(url.split(\"/\")[:3])\n",
    "    \n",
    "    def check_valid(urls):\n",
    "        '''Only full links containing base url'''\n",
    "        base_url_name = base_url.split(\"/\")[-1].split(\".\")[0]\n",
    "        valid = list(filter(lambda x: base_url_name in x, urls))\n",
    "        wo_base_url = list(filter(lambda x: base_url_name not in x and \"http\" not in x, urls))\n",
    "        ext = []\n",
    "        for link in wo_base_url:\n",
    "            if link.startswith(\"/\"):\n",
    "                ext.append(base_url + link)\n",
    "            else:\n",
    "                ext.append(f\"{base_url}/{link}\")\n",
    "        return valid + ext\n",
    "\n",
    "    def recursive_url(url):\n",
    "        '''Recursively finds the urls'''\n",
    "        try:\n",
    "            page = requests.get(url).text\n",
    "        except:\n",
    "            page = requests.get(base_url + url).text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "        return set([item.get('href', '') for item in soup.find_all('a')])\n",
    "\n",
    "\n",
    "    def get_links(url):\n",
    "        page = requests.get(url).text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "        links = set([item.get('href', '') for item in soup.find_all('a')])\n",
    "        new_links = links\n",
    "        for link in links.copy():\n",
    "            try:\n",
    "                new_links.update(recursive_url(link))\n",
    "            except: pass\n",
    "        return check_valid(new_links)\n",
    "    \n",
    "    return check_valid(get_links(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T21:41:32.581886Z",
     "start_time": "2017-08-13T21:41:32.560849Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "publics = [60114472, 19191317, 29534144, 23482909, 62438886, 81354264, \n",
    "           \"http://www.stplan.ru/\", \"http://www.30n.ru/2/1.html\", 65974416, 79084019, \n",
    "           \"http://www.fcpir.ru/\", \"https://4science.ru/\", 67183197, 66233468, \n",
    "           30558759, 98643656, 23509868, 56821139, 78860407, 61490488,\n",
    "           \"http://www.nsh.ru/\", \"http://россельхоз.рф/\", \n",
    "           \"http://be5.biz/upravlenie/gosudarstvennoe_upravlenie.html\", 97296142,\n",
    "           74686342, 79925455, 98643656, 63337812, 37959220, 10933209,\n",
    "           \"http://www.soldiering.ru\", \"https://voennoe-delo.com\",\n",
    "           \"https://www.cbr.ru/sbrfr/archive/fsfr/ffms/ru/legislation/corp_management_study/index.html\", 69693893,\n",
    "           49582956, 72388807, 30713157, 26978036, 69560028, 73537456, \n",
    "           29809500, 128350290, 37876217, 3800580\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T21:41:32.835388Z",
     "start_time": "2017-08-13T21:41:32.818232Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sources = {'art_test': 'TEST_ART', 'art_train': 'TRAIN_ART',\n",
    "           'politics_test': 'TEST_POLITICS', 'politics_train': 'TRAIN_POLITICS',\n",
    "           'finances_test': 'TEST_FINANCES', 'finances_train': 'TRAIN_FINANCES',\n",
    "           'strateg_management_test': 'TEST_STRATEG_MANAGEMENT', 'strateg_management_train': 'TRAIN_STRATEG_MANAGEMENT',\n",
    "           'law_test': 'TEST_LAW', 'law_train': 'TRAIN_LAW',\n",
    "           'elaboration_test': 'TEST_ELABORATION', 'elaboration_train': 'TRAIN_ELABORATION',\n",
    "           'industry_test': 'TEST_INDUSTRY', 'industry_train': 'TRAIN_INDUSTRY',\n",
    "           'education_test': 'TEST_EDUCATION', 'education_train': 'TRAIN_EDUCATION',\n",
    "           'social_business_test': 'TEST_SOCIAL_BUSINESS', 'social_business_train': 'TRAIN_SOCIAL_BUSINESS',\n",
    "           'public_health_test': 'TEST_PUBLIC_HEALTH', 'public_health_train': 'TRAIN_PUBLIC_HEALTH',\n",
    "           'agriculture_test': 'TEST_AGRICULTURE', 'agriculture_train': 'TRAIN_AGRICULTURE',\n",
    "           'government_management_test': 'TEST_GOVERNMENT_MANAGEMENT', 'government_management_train': 'TRAIN_GOVERNMENT_MANAGEMENT',\n",
    "           'smm_test': 'TEST_SMM', 'smm_train': 'TRAIN_SMM',\n",
    "           'innovations_test': 'TEST_INNOVATIONS', 'innovations_train': 'TRAIN_INNOVATIONS',\n",
    "           'safety_test': 'TEST_SAFETY', 'safety_train': 'TRAIN_SAFETY',\n",
    "           'military_test': 'TEST_MILITARY', 'military_train': 'TRAIN_MILITARY',\n",
    "           'corporative_management_test': 'TEST_CORPORATIVE_MANAGEMENT', 'corporative_management_train': 'TRAIN_CORPORATIVE_MANAGEMENT',\n",
    "           'social_safety_test': 'TEST_SOCIAL_SAFETY', 'social_safety_train': 'TRAIN_SOCIAL_SAFETY',\n",
    "           'building_test': 'TEST_BUILDING', 'building_train': 'TRAIN_BUILDING',\n",
    "           'entrepreneurship_test': 'TEST_ENTREPRENEURSHIP', 'entrepreneurship_train': 'TRAIN_ENTREPRENEURSHIP',\n",
    "           'sport_test': 'TEST_SPORT', 'sport_train': 'TRAIN_SPORT',\n",
    "           'investitions_test': 'TEST_INVESTITIONS', 'investitions_train': 'TRAIN_INVESTITIONS'\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T23:57:16.272273Z",
     "start_time": "2017-08-13T23:19:26.024749Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [12:51<00:00,  2.59it/s] \n",
      "100%|██████████| 2000/2000 [23:11<00:00,  1.44it/s]  \n"
     ]
    }
   ],
   "source": [
    "# get data for corpora's test\n",
    "for tag, id_ in zip(list(sources.keys())[::2], publics[::2]):\n",
    "    path = f\"corpora_test/{tag}.txt\"\n",
    "    s = set()\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, \"w\") as f:\n",
    "            if isinstance(id_, int):\n",
    "                wall = getallwall({\"owner_id\": -id_}, 2000)\n",
    "                for post in tqdm.tqdm(wall):\n",
    "                    if len(post) and post not in s:\n",
    "                        s.add(post)\n",
    "                        _ = f.write(f\"{post}\\n\")\n",
    "            elif isinstance(id_, str):\n",
    "                links = np.random.choice(list(get_all_links(id_)), 2000)\n",
    "                for link in tqdm.tqdm(links):\n",
    "                    try:\n",
    "                        page = requests.get(link).text\n",
    "                        soup = BeautifulSoup(page, \"lxml\")\n",
    "                        for text in soup.text.strip().split(\"\\n\"):\n",
    "                            if len(text) and text not in s:\n",
    "                                s.add(text)\n",
    "                                _ = f.write(f\"{text}\\n\")\n",
    "                    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-14T00:55:07.969018Z",
     "start_time": "2017-08-14T00:03:43.162312Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:16, 124.23it/s]\n",
      "2000it [00:13, 145.49it/s]\n",
      "2000it [00:11, 174.39it/s]\n",
      "100%|██████████| 2000/2000 [09:47<00:00,  1.68it/s]\n",
      "1754it [00:07, 225.30it/s]\n",
      "100%|██████████| 2000/2000 [25:16<00:00,  1.29it/s]  \n",
      "1976it [00:13, 150.12it/s]\n",
      "1724it [00:06, 256.14it/s]\n",
      "1449it [00:10, 140.87it/s]\n",
      "2000it [00:16, 124.86it/s]\n",
      "100%|██████████| 2000/2000 [06:45<00:00,  4.93it/s]  \n",
      "740it [00:04, 174.93it/s]\n",
      "2000it [00:08, 245.26it/s]\n",
      "2000it [00:07, 261.04it/s]\n",
      "2000it [00:07, 256.32it/s]\n",
      "100%|██████████| 2000/2000 [05:11<00:00,  5.02it/s]\n",
      "1008it [00:04, 234.19it/s]\n",
      "669it [00:02, 287.75it/s]\n",
      "2000it [00:11, 178.09it/s]\n",
      "2000it [00:16, 120.10it/s]\n",
      "2000it [00:09, 220.96it/s]\n",
      "2000it [00:09, 202.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# get data for corpora's train\n",
    "for tag, id_ in zip(list(sources.keys())[1::2], publics[1::2]):\n",
    "    path = f\"corpora_train/{tag}.txt\"\n",
    "    s = set()\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, \"w\") as f:\n",
    "            if isinstance(id_, int):\n",
    "                wall = getallwall({\"owner_id\": -id_}, 2000)\n",
    "                for post in tqdm.tqdm(wall):\n",
    "                    if len(post) and post not in s:\n",
    "                        s.add(post)\n",
    "                        _ = f.write(f\"{post}\\n\")\n",
    "            elif isinstance(id_, str):\n",
    "                links = np.random.choice(list(get_all_links(id_)), 2000)\n",
    "                for link in tqdm.tqdm(links):\n",
    "                    try:\n",
    "                        page = requests.get(link).text\n",
    "                        soup = BeautifulSoup(page, \"lxml\")\n",
    "                        for text in soup.text.strip().split(\"\\n\"):\n",
    "                            if len(text) and text not in s:\n",
    "                                s.add(text)\n",
    "                                _ = f.write(f\"{text}\\n\")\n",
    "                    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T10:53:19.109677Z",
     "start_time": "2017-08-12T10:53:19.105314Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ordered_sources = list(sources.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
