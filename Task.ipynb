{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T22:39:45.599797Z",
     "start_time": "2017-08-19T22:39:05.068289Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from util import *\n",
    "from scipy.sparse import vstack, csr_matrix, coo_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-19T22:50:00.962111Z",
     "start_time": "2017-08-19T22:50:00.935330Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sources = {'art': [60114472, 19191317], \n",
    "           'politics': [29534144, 23482909], \n",
    "           'finances': [62438886, 81354264],\n",
    "           'strateg_management': [\"http://www.stplan.ru/\", \"http://www.30n.ru/2/1.html\"], \n",
    "           'law': [65974416, 79084019],\n",
    "           'elaboration': [\"http://www.fcpir.ru/\", \"https://4science.ru/\"], \n",
    "           'industry': [67183197, 66233468], \n",
    "           'education': [30558759, 98643656],\n",
    "           'social_business': [23509868, 56821139],\n",
    "           'public_health': [78860407, 61490488],\n",
    "           'agriculture': [\"http://www.nsh.ru/\", \"http://россельхоз.рф/\"],\n",
    "           'government_management': [\"http://be5.biz/upravlenie/gosudarstvennoe_upravlenie.html\", 97296142],\n",
    "           'smm': [74686342, 79925455],\n",
    "           'innovations': [98643656, 63337812],\n",
    "           'safety': [37959220, 10933209],\n",
    "           'military': [\"http://www.soldiering.ru\", \"https://voennoe-delo.com\"],\n",
    "           'corporative_management': [\"https://www.cbr.ru/sbrfr/archive/fsfr/ffms/ru/legislation/corp_management_study/index.html\", 69693893],\n",
    "           'social_safety': [49582956, 72388807],\n",
    "           'building': [30713157, 26978036],\n",
    "           'entrepreneurship': [69560028, 73537456],\n",
    "           'sport': [29809500, 128350290],\n",
    "           'investitions': [37876217, 3800580]\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T23:57:16.272273Z",
     "start_time": "2017-08-13T23:19:26.024749Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [12:51<00:00,  2.59it/s] \n",
      "100%|██████████| 2000/2000 [23:11<00:00,  1.44it/s]  \n"
     ]
    }
   ],
   "source": [
    "# getting data for corpora\n",
    "for tag, ids_ in sources.items():\n",
    "    path = f\"assets/corpora/{tag}.txt\"\n",
    "    s = set()\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, \"w\") as f:\n",
    "            for id_ in ids_:\n",
    "                if isinstance(id_, int):\n",
    "                    wall = ParseClass.getallwall({\"owner_id\": -id_}, 2000)\n",
    "                    for post in tqdm.tqdm(wall):\n",
    "                        if len(post) and post not in s:\n",
    "                            s.add(post)\n",
    "                            _ = f.write(f\"{post}\\n\")\n",
    "                elif isinstance(id_, str):\n",
    "                    links = np.random.choice(list(ParseClass.get_all_links(id_)), 2000)\n",
    "                    for link in tqdm.tqdm(links):\n",
    "                        try:\n",
    "                            page = requests.get(link).text\n",
    "                            soup = BeautifulSoup(page, \"lxml\")\n",
    "                            for text in soup.text.strip().split(\"\\n\"):\n",
    "                                if len(text) and text not in s:\n",
    "                                    s.add(text)\n",
    "                                    _ = f.write(f\"{text}\\n\")\n",
    "                        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-12T10:53:19.109677Z",
     "start_time": "2017-08-12T10:53:19.105314Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ordered_sources = list(sources.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora_class = CorporaClass()\n",
    "\n",
    "for filename in tqdm.tqdm(os.listdir(\"assets/corpora\")):\n",
    "    with open(f\"assets/corpora/{filename}\") as f:\n",
    "        corpora_class.add_to_corpora(f)\n",
    "corpora_class.process_corpora()\n",
    "\n",
    "json.dump(list(corpora_class.vocab), open(\"vocab.json\", \"w\"))\n",
    "pickle.dump(corpora_class.corpora, open(\"corpora.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora = pickle.load(open(\"corpora.p\", \"rb\"))\n",
    "vocab = json.load(open(\"vocab.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(vocabulary=vocab)\n",
    "vectors = []\n",
    "for texts in tqdm.tqdm(corpora):\n",
    "    try:\n",
    "        vector = vectorizer.fit_transform(texts)\n",
    "    except:\n",
    "        vector = []\n",
    "    vectors.append(vector)\n",
    "    \n",
    "pickle.dump(vectors, open(\"vectors.p\", \"wb\"))\n",
    "pickle.dump(vectorizer, open(\"vectorizer.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = pickle.load(open(\"vectors.p\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DELIM = 5000\n",
    "NUM_OF_CLASSES = 22\n",
    "vector_size = vectors[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = []\n",
    "for i, item in enumerate(vectors):\n",
    "    if item != []:\n",
    "        num = min(item.shape[0], DELIM)\n",
    "        for _ in range(num):\n",
    "            y_ = np.zeros(NUM_OF_CLASSES)\n",
    "            y_[i] = 1\n",
    "            y.append(y_)\n",
    "y = np.array(y)\n",
    "\n",
    "num = min(vectors[0].shape[0], DELIM)\n",
    "X_temp = vectors[0][:num]\n",
    "for item in vectors[1:]:\n",
    "    if item != []:\n",
    "        num = min(item.shape[0], DELIM)\n",
    "        X_temp = vstack([X_temp, item[:num]], dtype='float64')\n",
    "X = X_temp\n",
    "\n",
    "y = csr_matrix(y)\n",
    "X = csr_matrix(X, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_acc',\n",
    "                           patience=8,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4,\n",
    "                           mode='max'),\n",
    "             ReduceLROnPlateau(monitor='val_acc',\n",
    "                               factor=0.1,\n",
    "                               patience=4,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4,\n",
    "                               mode='max'),\n",
    "             ModelCheckpoint(monitor='val_acc',\n",
    "                             filepath='squeeze_net_no_aug.h5',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=False,\n",
    "                             mode='max'),\n",
    "             # TensorBoard(log_dir='logs'),\n",
    "             CSVLogger('last_training_log.csv', separator=',', append=False)\n",
    "            ]\n",
    "\n",
    "# model.load_weights('squeeze_net_no_aug.h5')\n",
    "\n",
    "model.fit_generator(\n",
    "            generator=custom_train,\n",
    "            validation_data = custom_valid,    \n",
    "            # steps_per_epoch = 10,\n",
    "            steps_per_epoch=int(np.floor(train_image_count/batch_size)),\n",
    "            validation_steps =  int(np.floor(valid_image_count/batch_size)),\n",
    "            epochs=50,\n",
    "            callbacks=callbacks\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_vec = Input(shape=(vector_size,))\n",
    "l1 = Dense(96, activation='relu')(input_vec)\n",
    "l2 = Dense(18, activation='relu')(l1)\n",
    "l3 = Dropout(0.15)(l2)\n",
    "l4 = Dense(64, activation='sigmoid')(l3)\n",
    "output = Dense(NUM_OF_CLASSES, activation='softmax')(l4)\n",
    "\n",
    "classifier = Model(input_vec, output)\n",
    "classifier.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = X.toarray()\n",
    "y_ = y.toarray()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=196, epochs=250, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.save('vk_texts_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = json.load(open(\"fb_dump.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# social = pd.read_excel(\"social.xlsx\", names=['id', 'name', 'fb', 'ok', 'vk', 'email', 'twitter'])\n",
    "# social.set_index('id', inplace=True)\n",
    "# social = social[social.vk.notnull()]\n",
    "# social.vk = social.vk.apply(lambda x: str(x).split(\"/\")[-1])\n",
    "# df = social[social.vk.notnull()]\n",
    "# df = df[['name', 'vk']]\n",
    "\n",
    "# def get_id(screen_name):\n",
    "#     try:\n",
    "#         item = vk.users.get(user_ids=screen_name)\n",
    "#         if 'deactivated' not in item:\n",
    "#             return item[0]['id']\n",
    "#     except Exception as e:\n",
    "#         print(e.args)\n",
    "# # Нужно замапить screen_name с их vk_id, поэтому придется обрабатывать по одному\n",
    "\n",
    "# df['vk_id'] = df.vk.progress_apply(get_id)\n",
    "# df.dropna(inplace=True)\n",
    "# df.vk_id = df.vk_id.astype('int')\n",
    "# df.to_msgpack(\"df.msg\")\n",
    "df = pd.read_msgpack(\"df.msg\")\n",
    "known = pd.read_csv(\"known_users.csv\")\n",
    "known = known.merge(df[['vk_id']], left_on='Leader-ID', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_class = ResultClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offset = 0\n",
    "for index, row in tqdm.tqdm(known.iloc[offset:, :].iterrows(), total=len(known) - offset):\n",
    "    user_vk = row['vk_id']\n",
    "    if np.isnan(user_vk):\n",
    "        user_vk = None\n",
    "    user_fb = row['FB']\n",
    "    try:\n",
    "        verdict = result_class.get_result(user_vk, user_fb)\n",
    "        for cat, value in verdict:\n",
    "            known.loc[index, cat] = value\n",
    "    except Exception as e:\n",
    "        print(e.args)\n",
    "        for cat in categories:\n",
    "            known.loc[index, cat] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_names = dict(zip(categories, \n",
    "         [\"Искусство\", \"Политика\", \"Финансы\", \"Стратегическое управление\", \"Юриспруденция\", \"Исследования и разработки\",\n",
    "          \"Промышленность\", \"Образование\", \"Социальное предпринимательство\", \"Здравоохранение\", \"Сельское хозяйство\", \n",
    "          \"Государственное управление\", \"Реклама и маркетинг\", \"Инновации и модернизация\", \"Безопасность\", \n",
    "          \"Военное дело\", \"Корпоративное управление\", \"Социальная защита\", \"Строительство\", \"Предпринимательство\",\n",
    "          \"Спорт\", \"Инвестиции\"]))\n",
    "\n",
    "norm_dict = {\"values\": []}\n",
    "for name in known.ФИ:\n",
    "    results = []\n",
    "    for col in known[known.ФИ == name].iloc[:, 4:].columns:\n",
    "        results.append({\"name\": norm_names[col], \"value\": float(known.loc[known.ФИ == name, col].values[0])})\n",
    "    norm_dict['values'].append({\"name\": name, \"results\": results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "known.to_csv(\"known.csv\")\n",
    "json.dump(norm_dict, open(\"temporary_result.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
