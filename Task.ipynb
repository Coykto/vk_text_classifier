{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T07:57:33.371491Z",
     "start_time": "2017-08-20T07:57:08.604068Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from util import *\n",
    "from scipy.sparse import vstack, csr_matrix, coo_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import load_model\n",
    "from sklearn.utils import class_weight\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard, CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T07:57:33.400027Z",
     "start_time": "2017-08-20T07:57:33.373822Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sources = {'art': [60114472, 19191317], \n",
    "           'politics': [29534144, 23482909], \n",
    "           'finances': [62438886, 81354264],\n",
    "           'strateg_management': [\"http://www.stplan.ru/\", \"http://www.30n.ru/2/1.html\"], \n",
    "           'law': [65974416, 79084019],\n",
    "           'elaboration': [\"https://vk.com/\"], \n",
    "           'industry': [67183197, 66233468], \n",
    "           'education': [30558759, 98643656],\n",
    "           'social_business': [23509868, 56821139],\n",
    "           'public_health': [78860407, 61490488],\n",
    "           'agriculture': [\"http://www.nsh.ru/\", \"http://россельхоз.рф/\"],\n",
    "           'government_management': [\"http://be5.biz/upravlenie/gosudarstvennoe_upravlenie.html\", 97296142],\n",
    "           'smm': [74686342, 79925455],\n",
    "           'innovations': [98643656, 63337812],\n",
    "           'safety': [37959220, 10933209],\n",
    "           'military': [\"http://www.soldiering.ru\", \"https://voennoe-delo.com\"],\n",
    "           'corporative_management': [\"http://www.cfin.ru/management/\"],\n",
    "           'social_safety': [49582956, 72388807],\n",
    "           'building': [30713157, 26978036],\n",
    "           'entrepreneurship': [69560028, 73537456],\n",
    "           'sport': [29809500, 128350290],\n",
    "           'investitions': [37876217, 3800580]\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-20T07:57:33.437156Z",
     "start_time": "2017-08-20T07:57:33.402385Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting data for corpora\n",
    "for tag, ids_ in sources.items():\n",
    "    path = f\"assets/corpora/{tag}.txt\"\n",
    "    s = set()\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, \"w\") as f:\n",
    "            for id_ in ids_:\n",
    "                if isinstance(id_, int):\n",
    "                    wall = ParseClass.getallwall({\"owner_id\": -id_}, 1000)\n",
    "                    for post in tqdm.tqdm(wall):\n",
    "                        if len(post) and post not in s:\n",
    "                            s.add(post)\n",
    "                            _ = f.write(f\"{post}\\n\")\n",
    "                elif isinstance(id_, str):\n",
    "                    links = np.random.choice(list(ParseClass.get_all_links(id_)), 1000)\n",
    "                    for link in tqdm.tqdm(links):\n",
    "                        try:\n",
    "                            page = requests.get(link).text\n",
    "                            soup = BeautifulSoup(page, \"lxml\")\n",
    "                            for text in soup.text.strip().split(\"\\n\"):\n",
    "                                if len(text) and text not in s:\n",
    "                                    s.add(text)\n",
    "                                    _ = f.write(f\"{text}\\n\")\n",
    "                        except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:11.110Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = list(sources.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:12.093Z"
    }
   },
   "outputs": [],
   "source": [
    "corpora_class = CorporaClass()\n",
    "\n",
    "for filename in categories:\n",
    "    with open(f\"assets/corpora/{filename}.txt\") as f:\n",
    "        corpora_class.add_to_corpora(f)\n",
    "corpora_class.process_corpora()\n",
    "\n",
    "json.dump(list(corpora_class.vocab), open(\"assets/vocab.json\", \"w\"))\n",
    "pickle.dump(corpora_class.corpora, open(\"assets/corpora.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:12.777Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora = pickle.load(open(\"assets/corpora.p\", \"rb\"))\n",
    "vocab = json.load(open(\"assets/vocab.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:13.457Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(vocabulary=vocab)\n",
    "vectors = []\n",
    "for texts in tqdm.tqdm(corpora):\n",
    "    try:\n",
    "        vector = vectorizer.fit_transform(texts)\n",
    "    except:\n",
    "        vector = []\n",
    "    vectors.append(vector)\n",
    "    \n",
    "pickle.dump(vectors, open(\"assets/vectors.p\", \"wb\"))\n",
    "pickle.dump(vectorizer, open(\"assets/vectorizer.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:15.706Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = pickle.load(open(\"assets/vectors.p\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:22.950Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DELIM = 1500\n",
    "NUM_OF_CLASSES = 22\n",
    "vector_size = vectors[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:23.600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = []\n",
    "for i, item in enumerate(vectors):\n",
    "    if item != []:\n",
    "        num = min(item.shape[0], DELIM)\n",
    "        for _ in range(num):\n",
    "            y_ = np.zeros(NUM_OF_CLASSES)\n",
    "            y_[i] = 1\n",
    "            y.append(y_)\n",
    "y = np.array(y)\n",
    "\n",
    "num = min(vectors[0].shape[0], DELIM)\n",
    "X_temp = vectors[0][:num]\n",
    "for item in vectors[1:]:\n",
    "    if item != []:\n",
    "        num = min(item.shape[0], DELIM)\n",
    "        X_temp = vstack([X_temp, item[:num]], dtype='float64')\n",
    "X = X_temp\n",
    "\n",
    "y = csr_matrix(y)\n",
    "X = csr_matrix(X, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:24.693Z"
    }
   },
   "outputs": [],
   "source": [
    "input_vec = Input(shape=(vector_size,))\n",
    "l1 = Dense(96, activation='relu')(input_vec)\n",
    "l2 = Dense(18, activation='relu')(l1)\n",
    "l3 = Dropout(0.15)(l2)\n",
    "l4 = Dense(64, activation='sigmoid')(l3)\n",
    "output = Dense(NUM_OF_CLASSES, activation='sigmoid')(l4)\n",
    "\n",
    "classifier = Model(input_vec, output)\n",
    "classifier.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:26.332Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = X.toarray()\n",
    "y_ = y.toarray()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_, y_, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:27.384Z"
    }
   },
   "outputs": [],
   "source": [
    "# callbacks = [EarlyStopping(monitor='val_acc',\n",
    "#                            patience=8,\n",
    "#                            verbose=1,\n",
    "#                            min_delta=1e-4,\n",
    "#                            mode='max'),\n",
    "#              ReduceLROnPlateau(monitor='val_acc',\n",
    "#                                factor=0.1,\n",
    "#                                patience=4,\n",
    "#                                verbose=1,\n",
    "#                                epsilon=1e-4,\n",
    "#                                mode='max'),\n",
    "#              ModelCheckpoint(monitor='val_acc',\n",
    "#                              filepath='assets/vk_texts_classifier.h5',\n",
    "#                              save_best_only=True,\n",
    "#                              save_weights_only=False,\n",
    "#                              mode='max'),\n",
    "#              # TensorBoard(log_dir='logs'),\n",
    "#              CSVLogger('assets/last_training_log.csv', separator=',', append=False)\n",
    "#             ]\n",
    "\n",
    "# classifier.load_weights('vk_texts_classifier.h5')\n",
    "classifier.fit(X_train, \n",
    "               y_train, \n",
    "               validation_data=(X_val, y_val), \n",
    "               batch_size=196, \n",
    "               epochs=55, \n",
    "               class_weight='balanced'\n",
    "#                callbacks=callbacks\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:28.776Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.save(\"assets/vk_texts_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:30.845Z"
    }
   },
   "outputs": [],
   "source": [
    "# social = pd.read_excel(\"social.xlsx\", names=['id', 'name', 'fb', 'ok', 'vk', 'email', 'twitter'])\n",
    "# social.set_index('id', inplace=True)\n",
    "# social = social[social.vk.notnull()]\n",
    "# social.vk = social.vk.apply(lambda x: str(x).split(\"/\")[-1])\n",
    "# df = social[social.vk.notnull()]\n",
    "# df = df[['name', 'vk']]\n",
    "\n",
    "# def get_id(screen_name):\n",
    "#     try:\n",
    "#         item = vk.users.get(user_ids=screen_name)\n",
    "#         if 'deactivated' not in item:\n",
    "#             return item[0]['id']\n",
    "#     except Exception as e:\n",
    "#         print(e.args)\n",
    "# # Нужно замапить screen_name с их vk_id, поэтому придется обрабатывать по одному\n",
    "\n",
    "# df['vk_id'] = df.vk.progress_apply(get_id)\n",
    "# df.dropna(inplace=True)\n",
    "# df.vk_id = df.vk_id.astype('int')\n",
    "# df.to_msgpack(\"df.msg\")\n",
    "df = pd.read_msgpack(\"assets/df.msg\")\n",
    "known = pd.read_csv(\"assets/known_users.csv\")\n",
    "known = known.merge(df[['vk_id']], left_on='Leader-ID', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:31.668Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_class = ResultClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:32.824Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "offset = 0\n",
    "for index, row in tqdm.tqdm(known.iloc[offset:, :].iterrows(), total=len(known) - offset):\n",
    "    user_vk = row['vk_id']\n",
    "    if np.isnan(user_vk):\n",
    "        user_vk = None\n",
    "    user_fb = row['FB']\n",
    "    try:\n",
    "        verdict = result_class.get_result(user_vk, user_fb)\n",
    "        for cat, value in verdict:\n",
    "            known.loc[index, cat] = value\n",
    "    except ValueError:\n",
    "        for cat in categories:\n",
    "            known.loc[index, cat] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:34.180Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_names = dict(zip(sources.keys(), \n",
    "         [\"Искусство\", \"Политика\", \"Финансы\", \"Стратегическое управление\", \"Юриспруденция\", \"Исследования и разработки\",\n",
    "          \"Промышленность\", \"Образование\", \"Социальное предпринимательство\", \"Здравоохранение\", \"Сельское хозяйство\", \n",
    "          \"Государственное управление\", \"Реклама и маркетинг\", \"Инновации и модернизация\", \"Безопасность\", \n",
    "          \"Военное дело\", \"Корпоративное управление\", \"Социальная защита\", \"Строительство\", \"Предпринимательство\",\n",
    "          \"Спорт\", \"Инвестиции\"]))\n",
    "norm_names_reversed = dict([(a[1], a[0]) for a in norm_names.items()])\n",
    "\n",
    "norm_dict = {\"values\": []}\n",
    "for name in known.ФИ:\n",
    "    results = []\n",
    "    for col in known[known.ФИ == name].iloc[:, 4:].columns:\n",
    "        results.append({\"name\": norm_names[col], \"value\": float(known.loc[known.ФИ == name, col].values[0])})\n",
    "    norm_dict['values'].append({\"name\": name, \"results\": results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:42.392Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in known.iloc[:, 4:].columns:\n",
    "    norm_names[col], known.loc[:, col].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-20T07:57:43.037Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "known.to_csv(\"assets/known.csv\")\n",
    "json.dump(norm_dict, open(\"assets/temporary_result.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
